{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c48ff21",
   "metadata": {},
   "source": [
    "# Redis Write-Behind Pipeline\n",
    "This notebook implements inserting documents into Redis JSON, queuing them in Redis Streams, and writing to RDBMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9547f5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-10 19:06:17,662 [INFO] Connected to Redis at localhost:6379\n",
      "2025-08-10 19:06:17,696 [INFO] Connected to PostgreSQL at localhost:5432/redisdemo\n",
      "2025-08-10 19:06:17,698 [INFO] Creating search index\n",
      "2025-08-10 19:06:17,699 [WARNING] Could not create RediSearch index (idx:sessions). Error: Index already exists\n",
      "2025-08-10 19:06:17,699 [INFO] Starting periodic consumer for entity session, interval 10 seconds\n",
      "2025-08-10 19:06:17,700 [INFO] Starting dummy data generation\n",
      "2025-08-10 19:06:19,803 [WARNING] WAIT: fewer replicas acked (0) than requested (1)\n",
      "2025-08-10 19:06:21,935 [WARNING] WAIT: fewer replicas acked (0) than requested (1)\n",
      "2025-08-10 19:06:24,055 [WARNING] WAIT: fewer replicas acked (0) than requested (1)\n",
      "2025-08-10 19:06:26,178 [WARNING] WAIT: fewer replicas acked (0) than requested (1)\n",
      "2025-08-10 19:06:28,309 [WARNING] WAIT: fewer replicas acked (0) than requested (1)\n",
      "2025-08-10 19:06:28,385 [INFO] Dummy data generation complete\n",
      "2025-08-10 19:06:28,387 [INFO] Starting dummy searches\n",
      "2025-08-10 19:06:28,390 [ERROR] random_search_queries error: choice() takes 2 positional arguments but 6 were given\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/82/xg3dxm3j5p5_f58fcr87bh000000gn/T/ipykernel_54911/4119266571.py\", line 362, in random_search_queries\n",
      "    sample_field = random.choice(\"user_id:*\",\"device_type:{android}\",\"device_type:{web}\",\"device_type:{ios}\",\"device_model:{OnePlus-8}\")\n",
      "TypeError: choice() takes 2 positional arguments but 6 were given\n",
      "2025-08-10 19:06:28,393 [INFO] Starting metrics check\n",
      "2025-08-10 19:06:28,397 [INFO] Waiting briefly for consumer to process...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metrics:redis_success': '25', 'metrics:redis_fail': 0, 'metrics:redis_retry': 0, 'metrics:postgres_success': '23', 'metrics:postgres_fail': 0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 628\u001b[0m\n\u001b[1;32m    625\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShutdown complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 628\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 621\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;66;03m# Allow one last pass for consumer\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting briefly for consumer to process...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 621\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     stop_event\u001b[38;5;241m.\u001b[39mset()\n\u001b[1;32m    623\u001b[0m     consumer_thread\u001b[38;5;241m.\u001b[39mjoin(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "redis_writebehind_demo.py\n",
    "\n",
    "Single-file demo implementing:\n",
    " - Redis JSON storage (insert/update/delete)\n",
    " - Redis Streams for change events\n",
    " - Optional zero-data-loss (WAIT replicas + simulated AOF wait)\n",
    " - RediSearch index creation (if module present)\n",
    " - Dummy data generator (create/update/delete random sessions)\n",
    " - Read helpers (by ids and search)\n",
    " - RedisWriteBehind consumer which reads streams and writes to PostgreSQL with retry logic\n",
    " - Periodic consumer loop and metrics/logging into Redis\n",
    "\n",
    "Sample search queries:\n",
    "FT.SEARCH idx:sessions \"@user_id:user_4554\" RETURN 4 user_id session_id device_type device_model\n",
    "FT.SEARCH idx:sessions \"@device_type:{android}\" RETURN 4 user_id session_id device_type device_model\n",
    "\n",
    "Put a configs.yaml next to this file (example structure below). Example:\n",
    "-----------------------------------------------------------------------\n",
    "redis:\n",
    "  host: \"localhost\"\n",
    "  port: 6379\n",
    "  db: 0\n",
    "  password: null\n",
    "\n",
    "zero_data_loss:\n",
    "  enabled: true\n",
    "  wait_for_replicas: true\n",
    "  replicas: 1\n",
    "  wait_timeout_ms: 2000\n",
    "  wait_for_aof_rewrite: false   # best-effort simulation using BGREWRITEAOF\n",
    "\n",
    "streams:\n",
    "  prefix: \"stream\"\n",
    "  log_streams:\n",
    "    redis_success: \"log:redis:success\"\n",
    "    redis_fail: \"log:redis:fail\"\n",
    "    redis_retry: \"log:redis:retry\"\n",
    "    postgres_success: \"log:postgres:success\"\n",
    "    postgres_fail: \"log:postgres:fail\"\n",
    "  last_id_key_prefix: \"stream:lastid\"  # used to store last processed id per stream\n",
    "\n",
    "postgresql:\n",
    "  host: \"localhost\"\n",
    "  port: 5432\n",
    "  database: \"redisdemo\"\n",
    "  user: \"redis\"\n",
    "  password: \"redis\"\n",
    "\n",
    "dummy_data:\n",
    "  num_records: 100\n",
    "  create_pct: 0.6\n",
    "  update_pct: 0.3\n",
    "  delete_pct: 0.1\n",
    "  perform_write_behind:\n",
    "    insert: true\n",
    "    update: true\n",
    "    delete: false\n",
    "\n",
    "write_behind:\n",
    "  batch_size: 50\n",
    "  max_retry_attempts: 3\n",
    "  interval_seconds: 10\n",
    "\n",
    "search_index:\n",
    "  create_index: true\n",
    "  index_name: \"idx:sessions\"\n",
    "  fields:\n",
    "    - { name: \"user_id\", type: \"TEXT\" }\n",
    "    - { name: \"device_type\", type: \"TAG\" }\n",
    "    - { name: \"device_model\", type: \"TAG\" }\n",
    "-----------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "import threading\n",
    "import time\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import psycopg2\n",
    "import redis\n",
    "import yaml\n",
    "\n",
    "# ---------------------------\n",
    "# Logging setup\n",
    "# ---------------------------\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "logger = logging.getLogger(\"RedisWriteBehindDemo\")\n",
    "\n",
    "# ---------------------------\n",
    "# Utility / Config loading\n",
    "# ---------------------------\n",
    "def load_config(path: str = \"/Users/suyog/Documents/GitHub/redis-streams-to-rdbms-write-behind/configs/configs.yaml\") -> Dict[str, Any]:\n",
    "    with open(path, \"r\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    return cfg\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Connections\n",
    "# ---------------------------\n",
    "def connect_redis(cfg: Dict[str, Any]) -> redis.Redis:\n",
    "    rconf = cfg.get(\"redis\", {})\n",
    "    client = redis.Redis(\n",
    "        host=rconf.get(\"host\", \"localhost\"),\n",
    "        port=int(rconf.get(\"port\", 6379)),\n",
    "        password=rconf.get(\"password\", None),\n",
    "        decode_responses=True,\n",
    "    )\n",
    "    # Test connection\n",
    "    client.ping()\n",
    "    logger.info(\"Connected to Redis at %s:%s\", rconf.get(\"host\"), rconf.get(\"port\"))\n",
    "    return client\n",
    "\n",
    "\n",
    "def connect_postgres(cfg: Dict[str, Any]):\n",
    "    p = cfg.get(\"postgresql\", {})\n",
    "    conn = psycopg2.connect(\n",
    "        host=p.get(\"host\", \"localhost\"),\n",
    "        port=p.get(\"port\", 5432),\n",
    "        dbname=p.get(\"database\"),\n",
    "        user=p.get(\"user\"),\n",
    "        password=p.get(\"password\"),\n",
    "    )\n",
    "    conn.autocommit = True\n",
    "    logger.info(\"Connected to PostgreSQL at %s:%s/%s\", p.get(\"host\"), p.get(\"port\"), p.get(\"database\"))\n",
    "    return conn\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# ID generation\n",
    "# ---------------------------\n",
    "def generate_doc_id(app_id: str = \"app\", machine_id: Optional[str] = None) -> str:\n",
    "    if machine_id is None:\n",
    "        machine_id = uuid.getnode() & 0xFFFFFF\n",
    "    epoch = int(time.time() * 1000)\n",
    "    unique = uuid.uuid4().hex[:8]\n",
    "    return f\"{app_id}:{epoch}:{machine_id}:{unique}\"\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Redis JSON CRUD + Stream push + durability wait helpers\n",
    "# ---------------------------\n",
    "def _push_stream(redis_client: redis.Redis, cfg: Dict[str, Any], entity: str, doc_id: str, event_type: str):\n",
    "    streams_cfg = cfg.get(\"streams\", {})\n",
    "    prefix = streams_cfg.get(\"prefix\", \"stream\")\n",
    "    stream_name = f\"{prefix}:{entity}\"\n",
    "    payload = {\"ts\": str(int(time.time() * 1000)), \"doc_id\": doc_id, \"event_type\": event_type}\n",
    "    redis_client.xadd(stream_name, payload)\n",
    "    logger.debug(\"Pushed to stream %s: %s\", stream_name, payload)\n",
    "\n",
    "\n",
    "def _log_event(redis_client: redis.Redis, cfg: Dict[str, Any], key: str, payload: Dict[str, str]):\n",
    "    \"\"\"\n",
    "    Log into a configured Redis log stream and increment counters.\n",
    "    \"\"\"\n",
    "    streams_cfg = cfg.get(\"streams\", {})\n",
    "    log_streams = streams_cfg.get(\"log_streams\", {})\n",
    "    stream_name = log_streams.get(key)\n",
    "    if stream_name:\n",
    "        redis_client.xadd(stream_name, payload)\n",
    "    # increment counters\n",
    "    counter_key = f\"metrics:{key}\"\n",
    "    redis_client.incr(counter_key)\n",
    "\n",
    "\n",
    "def _wait_for_durability(redis_client: redis.Redis, cfg: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Implement 'zero data loss' durability best-effort:\n",
    "     - Wait for replica acknowledgement using WAIT command (replicas, timeout_ms)\n",
    "     - Optionally trigger BGREWRITEAOF and wait until rewrite completes (simulation of AOF persistence)\n",
    "    NOTE: There isn't a single immediate 'fsync AOF for a single write' Redis command; this is a best-effort approach.\n",
    "    \"\"\"\n",
    "    zdl = cfg.get(\"zero_data_loss\", {}) or {}\n",
    "    if not zdl.get(\"enabled\"):\n",
    "        return\n",
    "\n",
    "    # Wait for replicas\n",
    "    if zdl.get(\"wait_for_replicas\"):\n",
    "        replicas = int(zdl.get(\"replicas\", 1))\n",
    "        timeout_ms = int(zdl.get(\"wait_timeout_ms\", 2000))\n",
    "        try:\n",
    "            # WAIT returns number of replicas that acknowledged\n",
    "            acked = redis_client.execute_command(\"WAIT\", replicas, timeout_ms)\n",
    "            logger.debug(\"WAIT acked replicas: %s (requested %s)\", acked, replicas)\n",
    "            if acked < replicas:\n",
    "                logger.warning(\"WAIT: fewer replicas acked (%s) than requested (%s)\", acked, replicas)\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error while calling WAIT: %s\", e)\n",
    "\n",
    "    # Optionally simulate AOF durability by performing BGREWRITEAOF and waiting for completion\n",
    "    if zdl.get(\"wait_for_aof_rewrite\"):\n",
    "        try:\n",
    "            info = redis_client.info(section=\"persistence\")\n",
    "            rewrite_in_progress = info.get(\"aof_rewrite_in_progress\", 0)\n",
    "            if rewrite_in_progress == 0:\n",
    "                redis_client.bgrewriteaof()\n",
    "                # Poll until rewrite completes or times out\n",
    "                wait_start = time.time()\n",
    "                timeout = zdl.get(\"aof_rewrite_timeout_secs\", 30)\n",
    "                while True:\n",
    "                    time.sleep(0.5)\n",
    "                    info = redis_client.info(section=\"persistence\")\n",
    "                    if not info.get(\"aof_rewrite_in_progress\", 0):\n",
    "                        break\n",
    "                    if time.time() - wait_start > timeout:\n",
    "                        logger.warning(\"Timeout waiting for AOF rewrite to complete\")\n",
    "                        break\n",
    "                logger.debug(\"BGREWRITEAOF finished or timed out\")\n",
    "            else:\n",
    "                logger.debug(\"AOF rewrite already in progress\")\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error while triggering/waiting for BGREWRITEAOF: %s\", e)\n",
    "\n",
    "\n",
    "def insert_document(redis_client: redis.Redis, cfg: Dict[str, Any], entity: str, document: Dict[str, Any], doc_id: Optional[str] = None):\n",
    "    \"\"\"Insert JSON doc into Redis and push event to stream\"\"\"\n",
    "    if doc_id is None:\n",
    "        doc_id = generate_doc_id(app_id=entity)\n",
    "    key = f\"{entity}:{doc_id}\"\n",
    "    # JSON.SET key . <json>\n",
    "    try:\n",
    "        # Use JSON.SET if module exists\n",
    "        try:\n",
    "            redis_client.execute_command(\"JSON.SET\", key, \".\", json.dumps(document))\n",
    "        except redis.ResponseError as e:\n",
    "            # maybe JSON module not present; fallback to plain set\n",
    "            logger.debug(\"JSON.SET failed, falling back to plain SET: %s\", e)\n",
    "            redis_client.set(key, json.dumps(document))\n",
    "        # durability wait (replica ack / optional AOF)\n",
    "        _wait_for_durability(redis_client, cfg)\n",
    "        # push to stream\n",
    "        if cfg.get(\"dummy_data\", {}).get(\"perform_write_behind\", {}).get(\"insert\", True):\n",
    "            _push_stream(redis_client, cfg, entity, doc_id, \"insert\")\n",
    "        # logging\n",
    "        _log_event(redis_client, cfg, \"redis_success\", {\"ts\": str(int(time.time() * 1000)), \"key\": key, \"op\": \"insert\"})\n",
    "        return doc_id\n",
    "    except Exception as e:\n",
    "        logger.exception(\"insert_document failed: %s\", e)\n",
    "        _log_event(redis_client, cfg, \"redis_fail\", {\"ts\": str(int(time.time() * 1000)), \"key\": key, \"op\": \"insert\", \"err\": str(e)})\n",
    "        raise\n",
    "\n",
    "\n",
    "def update_document(redis_client: redis.Redis, cfg: Dict[str, Any], entity: str, doc_id: str, patch: Dict[str, Any]):\n",
    "    \"\"\"Update JSON doc fields (for simplicity we fully replace object)\"\"\"\n",
    "    key = f\"{entity}:{doc_id}\"\n",
    "    try:\n",
    "        # read existing\n",
    "        try:\n",
    "            curr = redis_client.execute_command(\"JSON.GET\", key, \".\")\n",
    "            curr_obj = json.loads(curr) if curr else {}\n",
    "        except redis.ResponseError:\n",
    "            curr_raw = redis_client.get(key)\n",
    "            curr_obj = json.loads(curr_raw) if curr_raw else {}\n",
    "        # apply patch (merge)\n",
    "        curr_obj.update(patch)\n",
    "        try:\n",
    "            redis_client.execute_command(\"JSON.SET\", key, \".\", json.dumps(curr_obj))\n",
    "        except redis.ResponseError:\n",
    "            redis_client.set(key, json.dumps(curr_obj))\n",
    "        _wait_for_durability(redis_client, cfg)\n",
    "        if cfg.get(\"dummy_data\", {}).get(\"perform_write_behind\", {}).get(\"update\", True):\n",
    "            _push_stream(redis_client, cfg, entity, doc_id, \"update\")\n",
    "        _log_event(redis_client, cfg, \"redis_success\", {\"ts\": str(int(time.time() * 1000)), \"key\": key, \"op\": \"update\"})\n",
    "    except Exception as e:\n",
    "        logger.exception(\"update_document failed: %s\", e)\n",
    "        _log_event(redis_client, cfg, \"redis_fail\", {\"ts\": str(int(time.time() * 1000)), \"key\": key, \"op\": \"update\", \"err\": str(e)})\n",
    "        raise\n",
    "\n",
    "\n",
    "def delete_document(redis_client: redis.Redis, cfg: Dict[str, Any], entity: str, doc_id: str):\n",
    "    key = f\"{entity}:{doc_id}\"\n",
    "    try:\n",
    "        # Delete JSON\n",
    "        try:\n",
    "            redis_client.execute_command(\"JSON.DEL\", key, \".\")\n",
    "        except redis.ResponseError:\n",
    "            redis_client.delete(key)\n",
    "        _wait_for_durability(redis_client, cfg)\n",
    "        if cfg.get(\"dummy_data\", {}).get(\"perform_write_behind\", {}).get(\"delete\", True):\n",
    "            _push_stream(redis_client, cfg, entity, doc_id, \"delete\")\n",
    "        _log_event(redis_client, cfg, \"redis_success\", {\"ts\": str(int(time.time() * 1000)), \"key\": key, \"op\": \"delete\"})\n",
    "    except Exception as e:\n",
    "        logger.exception(\"delete_document failed: %s\", e)\n",
    "        _log_event(redis_client, cfg, \"redis_fail\", {\"ts\": str(int(time.time() * 1000)), \"key\": key, \"op\": \"delete\", \"err\": str(e)})\n",
    "        raise\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Read helpers + search\n",
    "# ---------------------------\n",
    "def read_by_ids(redis_client: redis.Redis, entity: str, doc_ids: List[str]) -> List[Dict[str, Any]]:\n",
    "    docs = []\n",
    "    for did in doc_ids:\n",
    "        key = f\"{entity}:{did}\"\n",
    "        try:\n",
    "            try:\n",
    "                raw = redis_client.execute_command(\"JSON.GET\", key, \".\")\n",
    "            except redis.ResponseError:\n",
    "                raw = redis_client.get(key)\n",
    "            if raw:\n",
    "                # if JSON.GET returned a JSON string it may be double-quoted; attempt to parse\n",
    "                try:\n",
    "                    obj = json.loads(raw)\n",
    "                except Exception:\n",
    "                    # sometimes JSON.GET returns JSON string, try removing quotes\n",
    "                    obj = json.loads(raw.strip('\"'))\n",
    "                docs.append({\"doc_id\": did, \"doc\": obj})\n",
    "            else:\n",
    "                docs.append({\"doc_id\": did, \"doc\": None})\n",
    "        except Exception as e:\n",
    "            logger.exception(\"read_by_ids error for %s: %s\", key, e)\n",
    "            docs.append({\"doc_id\": did, \"error\": str(e)})\n",
    "    return docs\n",
    "\n",
    "\n",
    "def create_search_index(redis_client: redis.Redis, cfg: Dict[str, Any], entity: str):\n",
    "    \"\"\"\n",
    "    Very simple create index using RediSearch if present.\n",
    "    TTL, field types should be adjusted for production.\n",
    "    \"\"\"\n",
    "    sconf = cfg.get(\"search_index\", {}) or {}\n",
    "    if not sconf.get(\"create_index\"):\n",
    "        logger.info(\"Search index creation disabled in config\")\n",
    "        return\n",
    "    index_name = sconf.get(\"index_name\", f\"idx:{entity}\")\n",
    "    # Build basic schema: TEXT fields for provided fields\n",
    "    fields = sconf.get(\"fields\", [])\n",
    "    # If FT.CREATE fails, catch and log\n",
    "    try:\n",
    "        # Try a JSON index FT.CREATE idx ON JSON PREFIX 1 entity: SCHEMA $.user_id AS user_id TEXT ...\n",
    "        schema_parts = []\n",
    "        for field in sconf.get(\"fields\", []):\n",
    "            name = field.get(\"name\")\n",
    "            ftype = field.get(\"type\")\n",
    "            schema_parts.extend([f\"$.{name}\", \"AS\", name, ftype])\n",
    "        # Execute FT.CREATE... (works if RediSearch v2+ with JSON support)\n",
    "        cmd = [\"FT.CREATE\", index_name, \"ON\", \"JSON\", \"PREFIX\", \"1\", f\"{entity}:\", \"SCHEMA\"] + schema_parts\n",
    "        redis_client.execute_command(*cmd)\n",
    "        logger.info(\"Created RediSearch index %s for entity %s\", index_name, entity)\n",
    "    except redis.ResponseError as e:\n",
    "        # Could already exist, or module not available\n",
    "        logger.warning(\"Could not create RediSearch index (%s). Error: %s\", index_name, e)\n",
    "\n",
    "\n",
    "def random_search_queries(redis_client: redis.Redis, cfg: Dict[str, Any], entity: str, qcount: int = 5):\n",
    "    \"\"\"\n",
    "    Run a few sample search queries using FT.SEARCH if available.\n",
    "    This function is best-effort: if RediSearch isn't present it will log and return empty.\n",
    "    \"\"\"\n",
    "    sconf = cfg.get(\"search_index\", {}) or {}\n",
    "    index_name = sconf.get(\"index_name\", f\"idx:{entity}\")\n",
    "    results = []\n",
    "    try:\n",
    "        for _ in range(qcount):\n",
    "            # simple random sample: search by device_type or user_id patterns\n",
    "            search_list=[\"user_id:*\",\"device_type:{android}\",\"device_type:{web}\",\"device_type:{ios}\",\"device_model:{OnePlus-8}\"]\n",
    "            sample_field = random.choice(search_list)\n",
    "            # attempt to run FT.SEARCH\n",
    "            try:\n",
    "                resp = redis_client.execute_command(\"FT.SEARCH\", index_name, f\"@{sample_field}\", \"LIMIT\", \"0\", \"10\")\n",
    "                results.append(resp)\n",
    "                logger.debug(results)\n",
    "            except Exception as e:\n",
    "                logger.debug(\"FT.SEARCH failed: %s\", e)\n",
    "                break\n",
    "    except Exception as e:\n",
    "        logger.exception(\"random_search_queries error: %s\", e)\n",
    "    return results\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# RedisWriteBehind consumer\n",
    "# ---------------------------\n",
    "def _get_stream_names_for_entity(cfg: Dict[str, Any], entity: str) -> List[str]:\n",
    "    prefix = cfg.get(\"streams\", {}).get(\"prefix\", \"stream\")\n",
    "    return [f\"{prefix}:{entity}\"]\n",
    "\n",
    "\n",
    "def _get_last_processed_id(redis_client: redis.Redis, cfg: Dict[str, Any], stream_name: str) -> str:\n",
    "    key = f\"{cfg.get('streams',{}).get('last_id_key_prefix','stream:lastid')}:{stream_name}\"\n",
    "    val = redis_client.get(key)\n",
    "    return val or \"0-0\"\n",
    "\n",
    "\n",
    "def _set_last_processed_id(redis_client: redis.Redis, cfg: Dict[str, Any], stream_name: str, last_id: str):\n",
    "    key = f\"{cfg.get('streams',{}).get('last_id_key_prefix','stream:lastid')}:{stream_name}\"\n",
    "    redis_client.set(key, last_id)\n",
    "\n",
    "\n",
    "def RedisWriteBehind(redis_client: redis.Redis, pg_conn, cfg: Dict[str, Any], entity: str):\n",
    "    \"\"\"\n",
    "    Read events from Redis Streams (range-based) and write to PostgreSQL with retry logic.\n",
    "    The function uses XRANGE starting from last-processed ID for each configured stream.\n",
    "    \"\"\"\n",
    "    wb_cfg = cfg.get(\"write_behind\", {})\n",
    "    batch_size = int(wb_cfg.get(\"batch_size\", 50))\n",
    "    max_retries = int(wb_cfg.get(\"max_retry_attempts\", 3))\n",
    "\n",
    "    stream_names = _get_stream_names_for_entity(cfg, entity)\n",
    "    for stream in stream_names:\n",
    "        last_id = _get_last_processed_id(redis_client, cfg, stream)\n",
    "        # XRANGE stream last_id + batch_size\n",
    "        # XRANGE expects start < end; to get > last_id, use (last_id\n",
    "        try:\n",
    "            entries = redis_client.xrange(stream, min=last_id, max=\"+\", count=batch_size)\n",
    "            # Note: xrange returns entries inclusive of start; we need to skip start if equals last_id\n",
    "            to_process = []\n",
    "            for eid, fields in entries:\n",
    "                if eid == last_id:\n",
    "                    continue\n",
    "                # fields is dict of bytes->str\n",
    "                # Standardize fields\n",
    "                ev = {k: v for k, v in fields.items()}\n",
    "                to_process.append((eid, ev))\n",
    "            if not to_process:\n",
    "                continue\n",
    "\n",
    "            # Now write each event to PostgreSQL\n",
    "            cur = pg_conn.cursor()\n",
    "            for eid, ev in to_process:\n",
    "                event_type = ev.get(\"event_type\")\n",
    "                doc_id = ev.get(\"doc_id\")\n",
    "                ts = ev.get(\"ts\")\n",
    "                key = f\"{entity}:{doc_id}\"\n",
    "                # Fetch current document from Redis if needed (for insert/update)\n",
    "                doc_obj = None\n",
    "                if event_type in (\"insert\", \"update\"):\n",
    "                    try:\n",
    "                        raw = redis_client.execute_command(\"JSON.GET\", key, \".\")\n",
    "                    except redis.ResponseError:\n",
    "                        raw = redis_client.get(key)\n",
    "                    if raw:\n",
    "                        try:\n",
    "                            doc_obj = json.loads(raw)\n",
    "                        except Exception:\n",
    "                            doc_obj = json.loads(raw.strip('\"'))\n",
    "                # Build SQL based on event type\n",
    "                sql = None\n",
    "                params = None\n",
    "                if event_type == \"insert\":\n",
    "                    # Assuming a table sessions(id text primary key, payload jsonb, created_at timestamptz)\n",
    "                    sql = \"INSERT INTO sessions (id, payload, created_at) VALUES (%s, %s::jsonb, to_timestamp(%s::double precision / 1000)) ON CONFLICT (id) DO UPDATE SET payload = EXCLUDED.payload\"\n",
    "                    params = (doc_id, json.dumps(doc_obj or {}), ts or int(time.time() * 1000))\n",
    "                elif event_type == \"update\":\n",
    "                    sql = \"UPDATE sessions SET payload = %s::jsonb WHERE id = %s\"\n",
    "                    params = (json.dumps(doc_obj or {}), doc_id)\n",
    "                elif event_type == \"delete\":\n",
    "                    sql = \"DELETE FROM sessions WHERE id = %s\"\n",
    "                    params = (doc_id,)\n",
    "                else:\n",
    "                    logger.warning(\"Unknown event_type: %s\", event_type)\n",
    "                    _log_event(redis_client, cfg, \"redis_fail\", {\"ts\": str(int(time.time() * 1000)), \"stream\": stream, \"id\": eid, \"op\": \"unknown_event\"})\n",
    "                    _set_last_processed_id(redis_client, cfg, stream, eid)\n",
    "                    continue\n",
    "\n",
    "                # Execute with retry\n",
    "                success = False\n",
    "                attempt = 0\n",
    "                while attempt <= max_retries and not success:\n",
    "                    try:\n",
    "                        cur.execute(sql, params)\n",
    "                        _log_event(redis_client, cfg, \"postgres_success\", {\"ts\": str(int(time.time() * 1000)), \"id\": doc_id, \"op\": event_type})\n",
    "                        success = True\n",
    "                    except Exception as e:\n",
    "                        attempt += 1\n",
    "                        logger.exception(\"Error writing to Postgres (attempt %s/%s): %s\", attempt, max_retries, e)\n",
    "                        _log_event(redis_client, cfg, \"postgres_fail\", {\"ts\": str(int(time.time() * 1000)), \"id\": doc_id, \"op\": event_type, \"err\": str(e)})\n",
    "                        if attempt <= max_retries:\n",
    "                            _log_event(redis_client, cfg, \"redis_retry\", {\"ts\": str(int(time.time() * 1000)), \"id\": doc_id, \"attempt\": str(attempt)})\n",
    "                            time.sleep(1)  # backoff; could be exponential\n",
    "                        else:\n",
    "                            logger.error(\"Exceeded max retries for id %s op %s; skipping\", doc_id, event_type)\n",
    "                # After processing (success or after exhausting retries) advance last_id\n",
    "                _set_last_processed_id(redis_client, cfg, stream, eid)\n",
    "            cur.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error in RedisWriteBehind for stream %s: %s\", stream, e)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Dummy data generator\n",
    "# ---------------------------\n",
    "def generate_dummy_session():\n",
    "    \"\"\"Return a sample session document dict\"\"\"\n",
    "    user_id = f\"user_{random.randint(1, 5000)}\"\n",
    "    device_type = random.choice([\"android\", \"ios\", \"web\"])\n",
    "    device_model = random.choice([\"Pixel-4\", \"iPhone-12\", \"Galaxy-S10\", \"OnePlus-8\"])\n",
    "    session_id = uuid.uuid4().hex\n",
    "    now = int(time.time() * 1000)\n",
    "    last_activity = now - random.randint(0, 300000)\n",
    "    login_time = now - random.randint(0, 3600000)\n",
    "    geo = {\"lat\": round(12 + random.random(), 6), \"lon\": round(77 + random.random(), 6)}\n",
    "    device_logs = [{\"ts\": now - i * 1000, \"evt\": random.choice([\"click\", \"tap\", \"swipe\"])} for i in range(random.randint(1, 5))]\n",
    "    return {\n",
    "        \"user_id\": user_id,\n",
    "        \"session_id\": session_id,\n",
    "        \"device_logs\": device_logs,\n",
    "        \"last_activity\": last_activity,\n",
    "        \"login_time\": login_time,\n",
    "        \"device_type\": device_type,\n",
    "        \"device_model\": device_model,\n",
    "        \"geo_location\": geo,\n",
    "    }\n",
    "\n",
    "\n",
    "def dummy_data_worker(redis_client: redis.Redis, cfg: Dict[str, Any], entity: str):\n",
    "    \"\"\"\n",
    "    Generate dummy records and perform create/update/delete operations randomly based on config percentages.\n",
    "    \"\"\"\n",
    "    dd = cfg.get(\"dummy_data\", {})\n",
    "    n = int(dd.get(\"num_records\", 100))\n",
    "    create_pct = float(dd.get(\"create_pct\", 0.6))\n",
    "    update_pct = float(dd.get(\"update_pct\", 0.3))\n",
    "    delete_pct = float(dd.get(\"delete_pct\", 0.1))\n",
    "    created_ids = []\n",
    "\n",
    "    for i in range(n):\n",
    "        r = random.random()\n",
    "        if r <= create_pct or not created_ids:\n",
    "            # create\n",
    "            doc = generate_dummy_session()\n",
    "            doc_id = insert_document(redis_client, cfg, entity, doc)\n",
    "            created_ids.append(doc_id)\n",
    "            logger.debug(\"Dummy created %s\", doc_id)\n",
    "        elif r <= create_pct + update_pct and created_ids:\n",
    "            # update a random existing\n",
    "            doc_id = random.choice(created_ids)\n",
    "            patch = {\"last_activity\": int(time.time() * 1000), \"device_logs\": [{\"ts\": int(time.time() * 1000), \"evt\": \"heartbeat\"}]}\n",
    "            update_document(redis_client, cfg, entity, doc_id, patch)\n",
    "            logger.debug(\"Dummy updated %s\", doc_id)\n",
    "        else:\n",
    "            # delete random\n",
    "            if created_ids:\n",
    "                doc_id = created_ids.pop(random.randrange(len(created_ids)))\n",
    "                delete_document(redis_client, cfg, entity, doc_id)\n",
    "                logger.debug(\"Dummy deleted %s\", doc_id)\n",
    "\n",
    "        # slight pause to emulate traffic\n",
    "        time.sleep(random.uniform(0.01, 0.1))\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Periodic consumer loop\n",
    "# ---------------------------\n",
    "def consume_periodically(redis_client: redis.Redis, pg_conn, cfg: Dict[str, Any], entity: str, stop_event: threading.Event):\n",
    "    interval = int(cfg.get(\"write_behind\", {}).get(\"interval_seconds\", 10))\n",
    "    logger.info(\"Starting periodic consumer for entity %s, interval %s seconds\", entity, interval)\n",
    "    while not stop_event.is_set():\n",
    "        try:\n",
    "            RedisWriteBehind(redis_client, pg_conn, cfg, entity)\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error in periodic RedisWriteBehind: %s\", e)\n",
    "        # sleep in small increments so stop_event is responsive\n",
    "        for _ in range(interval):\n",
    "            if stop_event.is_set():\n",
    "                break\n",
    "            time.sleep(1)\n",
    "    logger.info(\"Stopped periodic consumer for entity %s\", entity)\n",
    "\n",
    "# Load Metrics\n",
    "def load_metrics(r, config):\n",
    "    metric_keys = ['metrics:redis_success','metrics:redis_fail','metrics:redis_retry','metrics:postgres_success','metrics:postgres_fail']\n",
    "    return {key: r.get(key) or 0 for key in metric_keys}\n",
    "    \n",
    "# ---------------------------\n",
    "# Main\n",
    "# ---------------------------\n",
    "def main():\n",
    "    cfg = load_config(\"/Users/suyog/Documents/GitHub/redis-streams-to-rdbms-write-behind/configs/configs.yaml\")\n",
    "    redis_client = connect_redis(cfg)\n",
    "    pg_conn = connect_postgres(cfg)\n",
    "\n",
    "    entity = \"session\"  # can be config-driven\n",
    "\n",
    "    # Create Postgres table if not exists (simple schema)\n",
    "    try:\n",
    "        cur = pg_conn.cursor()\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS sessions (\n",
    "                id TEXT PRIMARY KEY,\n",
    "                payload JSONB,\n",
    "                created_at TIMESTAMPTZ DEFAULT now()\n",
    "            )\n",
    "            \"\"\"\n",
    "        )\n",
    "        cur.close()\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Error creating sessions table: %s\", e)\n",
    "        return\n",
    "\n",
    "    # Create search index if configured\n",
    "    logger.info(\"Creating search index\")\n",
    "    create_search_index(redis_client, cfg, entity)\n",
    "    # Start periodic consumer in background thread\n",
    "    stop_event = threading.Event()\n",
    "    consumer_thread = threading.Thread(target=consume_periodically, args=(redis_client, pg_conn, cfg, entity, stop_event), daemon=True)\n",
    "    consumer_thread.start()\n",
    "\n",
    "    # Run dummy data generator in main thread (or could be separate)\n",
    "    try:\n",
    "        logger.info(\"Starting dummy data generation\")\n",
    "        dummy_data_worker(redis_client, cfg, entity)\n",
    "        logger.info(\"Dummy data generation complete\")\n",
    "        logger.info(\"Starting dummy searches\")\n",
    "        random_search_queries(redis_client, cfg, entity, 5)\n",
    "        logger.info(\"Starting metrics check\")\n",
    "        print(load_metrics(redis_client, cfg))\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Interrupted by user\")\n",
    "    finally:\n",
    "        # Allow one last pass for consumer\n",
    "        logger.info(\"Waiting briefly for consumer to process...\")\n",
    "        time.sleep(100)\n",
    "        stop_event.set()\n",
    "        consumer_thread.join(timeout=10)\n",
    "        pg_conn.close()\n",
    "        logger.info(\"Shutdown complete\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914a1c8d-9158-4f76-b17a-28137e00f320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
